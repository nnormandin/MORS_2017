---
title: "Probability calibration methodologies with local expert ensembles"
subtitle: "CPT Nick Normandin"
output: 
  beamer_presentation: 
    colortheme: seahorse
    keep_tex: yes
    theme: Dresden
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = 'pdf')

setwd('/home/nick/R/projects/MORS_2017/')
source('./utils.R')
```

# Introduction

## What should you know about this brief?

> - **Please ask questions as I go**
> - I've assumed some audience proficiency in modern machine learning techniques, but I will alternatively try to provide a heuristic understanding of the concepts presented
> - Full accompanying R code is available
> - This work was funded by the Omar N. Bradley Officer Research Fellowship in Mathematics

## What is `local expert`?

I created a new kind of ensemble forecasting method called local expert regression. It involves the decomposition of a supervised learning task with a continuous target variable (*regression*) into a series of of many $\{0, 1\}$ mappings corresponding to separate *binary probabilistic classification* tasks that produce estimates on the $[0, 1]$ interval.

#####  Why is this useful ?! 
Because you can aggregate the ensemble predictions to form a completely unique *probability distribution function* for each prediction. You can understand **risk** not just in terms of a model, but in terms of each individual forecast.

#### ...see **[github.com/nnormandin/localexpeRt](https://github.com/nnormandin/localexpeRt)**

## What problem am I solving?

\begin{columns}
\begin{column}{0.48\textwidth}

Most classification methods produce scores for class membership which are interpereted as measures of class affiliation probability. This is the foundation of local expert regression. However, these `probabilities' are not usually \textbf{well-calibrated}. 

\end{column}


\begin{column}{0.48\textwidth}
\begin{block}{Definition:}

For a model $f$ and score $s_i$ to be well-calibrated for class $c_i$, the empirical probability of a correct classification $P(c_i | f( c_i | x_i)=s_i)$ must converge to $f(c_i | x_i) = s_i$.

\end{block}
\begin{block}{Example:}

When $s_i = 0.9$, the probability of a correct classification should converge to $P(c_i | s_i = 0.9) = 0.9$. Otherwise, this isn't \textit{really} a `probability.'

\end{block}
\end{column}
\end{columns}



## How do I propose to solve it?

#### If probabilities aren't properly calibrated, the PDFs interpolated from them won't be reliable. How can we deal with this?

1. Change the loss function
     + $n^{-1}\sum_{i=1}^{n}-y_i\log(p_i)-(1-y_i)\log(1-p_i)$
2. Calibrate probabilities
     + isotonic regression, sigmoid transforms?

# Local expert

## How is local expert different from normal regression?

![image](~/R/projects/MORS_2017/figure1/first.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/second.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/third.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/fourth.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/last.png)


## How is local expert different from normal regression?

- The local expert predictions can be reconstructed to form an empirical CDF
- By fitting a spline to these points and then differentiating, we can create a unique PDF for each prediction
- We can forecast $\hat{y}$ based on the moments of the distribution, or fit another meta-model on the output of the local expert ensemble (see Wolpert's work re: stacked generalization)
- **BUT,** all of these cool things we could do are dependent on having well-calibrated probability scores

# Probability calibration

## Why are some model scores poorly calibrated?

\begin{columns}[t]
\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0.5}
\begin{itemize}
\item Maximal margin hyperplanes push scores away from extremes of distribution
\item Common in support vector machines, boosted learners
\end{itemize}
\end{block}

\end{column}


\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0, 1}
\begin{itemize}
\item Model assumptions make class probabilites unrealistically confident
\item Naive Bayes!
\end{itemize}
\end{block}
\end{column}
\end{columns}


## How can we visualize calibration?

Cross-validated class probabilities from a naive bayes model trained on the Pima Indian Diabetes data 

\footnotesize
```{r, message=FALSE, warning=FALSE, cache = TRUE}
m <- train(x = PimaIndiansDiabetes[,1:8], y = PimaIndiansDiabetes[,9], tuneLength = 1,
           method = 'svmLinear', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

pred <- m$pred[order(m$pred$rowIndex),]

result <- data.table(prob = pred$pos,
                     class = ifelse(pred$obs == 'pos', 1, 0))

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')
```

The logarithmic loss here is `r MLmetrics::LogLoss(result$prob, result$class)`

## How can we visualize calibration?

Reliability plot: **(1)** Bin predictions by $s_i$ (x-axis), **(2)** calculate $p(c_i)$ by bin (y-axis)

```{r}
ReliabilityPlot(result$class, result$prob, bins = 10)
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r, cache = TRUE}
x <- seq(200)
y <- seq(200) + rnorm(200, 0, 30)
```

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
lines(isoreg(x,y), lwd = 4)
```

## Method 1: Isotonic Regression
Applying it to the Pima Indian Diabetes estimates from earlier

```{r}
iso_mod <- isoreg(x = result$prob, y = result$class)
ip <- setorder(data.table(iso_mod$yf, iso_mod$ord), V2)
result[, iso_probs := ip$V1]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(iso_mod, lwd = 4)

```

<!-- The logarithmic loss here is `r MLmetrics::LogLoss(result$iso_probs, result$class)` -->


## Method 2: Platt Scaling
Pass $s_i$ through the sigmoid

$$P(c_i | s_i) = \frac{1}{1 + \exp(As_i + B)}$$


where $A$ and $B$ are the solution to
$$\underset{A, B}{\operatorname{argmax}} - \sum\limits_{i} y_i \log(p_i) + (1 - y_i) \log(1- p_i)$$


## Method 2: Platt Scaling
Applying it to the Pima Indian Diabetes estimates from earlier


```{r}

pmod <- train(x = result[,1], y = ifelse(result$class == 1, 'y', 'n'), family = 'binomial',
           method = 'glm', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

result[, platt_probs := pmod$pred[order(pmod$pred$rowIndex),4]]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(y = sort(result$platt_probs), x = sort(result$prob), lwd = 4, col = 'red')
```

# Case study

## The task

- 30,000 records of 300+ variables related to Russian housing transactions
- very dirty, lots of multicollinearity

![image](~/R/projects/MORS_2017/figure1/kaggle.png)

## Preparing the data

I cleaned and pre-processed separately, so we'll just read in those files and partition the train and test sets.

\footnotesize
```{r, echo = TRUE, cache = TRUE}
X <- readRDS('./dataX')
y <- readRDS('./dataY')

trainset <- createDataPartition(y, p = 0.6)[[1]]

Xtrain <- X[trainset,]; Xtest <- X[-trainset,]
ytrain <- y[trainset]; ytest <- y[-trainset]
```

```{r, cache = TRUE}
tc <- trainControl(method = 'cv', number = 8, savePredictions = TRUE,
                   classProbs = TRUE, returnData = FALSE)
```


## COA 0: Tune and train a regression model
\footnotesize
```{r, echo = TRUE, message=FALSE, warning=FALSE, cache = TRUE}
mod0 <- train(x = Xtrain, y = ytrain, method = 'gbm',
              tuneLength = 5, trControl = tc,
              verbose = FALSE)

mod0_preds <- ExtractBestPreds(mod0)
mod0_preds[mod0_preds < 0] <- 0
```


## COA 1A: Use a local expert ensemble

Map the continuous `y` vector into a binary matrix

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
yb <- BinCols(ytrain, n = 25, mode = 'EP')
```

\normalsize
Induce separate models across each column in the matrix

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE, cache.lazy=FALSE}
LEs <- TrainLEs(x = Xtrain, bincols = yb$cols, trControl = tc,
                method = 'gbm', n.folds = 8, tuneLength = 8,
                verbose = FALSE)

LE_info <- ExtractModelInfo(LEs)
```

```{r, message=FALSE, warning=FALSE}
# save local experts so I don't have to compute again

try(save(LEs, file = '/home/nick/Desktop/LE_backup.RData'))

```


## COA 1A: Use a local expert ensemble

\footnotesize
```{r, echo=TRUE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
sink('/dev/null'); PlotLEs(LE_info); sink()
```

## COA 1A: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
tmp <- FitInstance(LE_info$preds.matrix[sample(1:20000, 1),], yb$y.vals, plot = FALSE)
sample.interp <- (tmp$sample.points[1:length(tmp$sample.points)-1]
                    + tmp$sample.points[2:length(tmp$sample.points)])/2
plot(x = sample.interp, y = tmp$epdf, type = 'l',
     xlab = 'target variable', ylab = 'probability', xlim = c(0, 2e07))

```


## COA 1A: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
tmp <- FitInstance(LE_info$preds.matrix[sample(1:20000, 1),], yb$y.vals, plot = FALSE)
sample.interp <- (tmp$sample.points[1:length(tmp$sample.points)-1]
                    + tmp$sample.points[2:length(tmp$sample.points)])/2
plot(x = sample.interp, y = tmp$epdf, type = 'l',
     xlab = 'target variable', ylab = 'probability', xlim = c(0, 2e07))
```

## COA 1A: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
tmp <- FitInstance(LE_info$preds.matrix[sample(1:20000, 1),], yb$y.vals, plot = FALSE)
sample.interp <- (tmp$sample.points[1:length(tmp$sample.points)-1]
                    + tmp$sample.points[2:length(tmp$sample.points)])/2
plot(x = sample.interp, y = tmp$epdf, type = 'l',
     xlab = 'target variable', ylab = 'probability', xlim = c(0, 2e07))
```

## COA 1A: Use a local expert ensemble
Predict $\hat{y}$ using just the means of the empirical distributions

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
LE_fits <- FitMatrix(LE_info$preds.matrix, yb$y.vals)
plot(x = ytrain, y = LE_fits$mean)
```


## COA 1B: 

Fit a stacked meta-model on the local expert layer output (include distribution mean as additional meta-feature).

```{r}
pm <- as.data.table(LE_info$preds.matrix)

tc1 <- trainControl(method = 'cv', number = 8, savePredictions = TRUE,
                   returnData = TRUE)
```

\footnotesize
```{r, echo = TRUE}
meta_X1 <- pm
meta_X1$mean <- LE_fits$mean

mod_1B <- train(x= meta_X1, y = ytrain, method = 'gbm',
                tuneLength = 10, trControl = tc1,
                verbose = FALSE)
```

```{r}
mod1B_preds <- ExtractBestPreds(mod_1B)
mod1B_preds[mod1B_preds < 0] <- 0
```


## COA 2A: Local expert with calibrated probabilities

The output of each local expert must be calibrated independently. Each has a distinct reliability plot, and class imbalance has a substantial effect on the probability scores generated.

```{r, fig.height=5, fig.width=10}
c5 <- ifelse(yb$cols$c5 == 'DOWN', 1, 0)
ReliabilityPlot(c5, pm$c5, bins = 10)
```

## COA 2A: Local expert with calibrated probabilities

The output of each local expert must be calibrated independently. Each has a distinct reliability plot, and class imbalance has a substantial effect on the probability scores generated.

```{r, fig.height=5, fig.width=10}
c3 <- ifelse(yb$cols$c3 == 'DOWN', 1, 0)
ReliabilityPlot(c3, pm$c5, bins = 10)
```

## COA 2A: Local expert with calibrated probabilities

Instead of simply fitting an isotonic regression model to our local expert output, we can remove some of the bumpiness of the step function and reduce overfitting by bootstrapping. The steps are:

1. Draw random sample with replacement
2. Fit isotonic step function using PAVA algorithm
3. Repeat steps 1 & 2 for 500 iterations
4. Average all step functions
5. Return final `step function' (still monotonic)

<!-- ## COA 2: Local expert with calibrated probabilities -->

```{r, message=FALSE, warning=FALSE}
iso_mods <- bootstrap_iso(pm, yb$cols, NBOOT = 500, SAMP = 0.3)
pm2 <- predict_bootiso(iso_mods, pm)

LE_fits2 <- FitMatrix(as.matrix(pm2), yb$y.vals)
```

## COA 2B: Stacked model with calibrated probabilities 

Fit a stacked meta-model on the local expert layer output (include distribution mean as additional meta-feature).

\footnotesize
```{r, echo = TRUE}
meta_X2 <- pm2
meta_X2$mean <- LE_fits2$mean

mod_2B <- train(x= meta_X2, y = ytrain, method = 'gbm',
                tuneLength = 8, trControl = tc1,
                verbose = FALSE)
```

```{r}
mod2B_preds <- ExtractBestPreds(mod_2B)
mod2B_preds[mod2B_preds < 0] <- 0
```

# Results

## Model CV and out-of-sample RMLSE

```{r, message=FALSE, warning=FALSE}
coa0_testpreds <- predict(mod0, newdata = Xtest)
coa0_testpreds[coa0_testpreds < 0] <- 0
coa0_oos <- MLmetrics::RMSE(log(1 + coa0_testpreds), log(1 + ytest))

LE_testpreds <- as.data.frame(PredictLEs(Xtest, LEs))
LE_testfits <- FitMatrix(as.data.frame(LE_testpreds), yb$y.vals)
LE_testfits$mean[LE_testfits$mean < 0] <- 0
coa1a_oos <- MLmetrics::RMSE(log(1 + LE_testfits$mean), log(1 + ytest))

testX1 <- LE_testpreds
testX1$mean <- LE_testfits$mean
coa1b_oos <- MLmetrics::RMSE(log(1 + predict(mod_1B, testX1)), log(1 + ytest))

LE_testpreds2 <- predict_bootiso(iso_mods, LE_testpreds)
LE_testfits2 <- FitMatrix(LE_testpreds2, yb$y.vals)
coa2a_oos <- MLmetrics::RMSE(log(1 + LE_testfits2$mean), log(1 + ytest))

testX2 <- LE_testpreds2
testX2$mean <- LE_testfits2$mean
coa2b_oos <- MLmetrics::RMSE(log(1 + predict(mod_2B, testX2)), log(1 + ytest))

```


```{r, message=FALSE, warning=FALSE}
coa1a_mse <- MLmetrics::RMSE(log(1 + LE_fits$mean), log(1 + ytrain))
coa1b_mse <- MLmetrics::RMSE(log(1 + mod1B_preds), log(1 + ytrain))

coa2a_mse <- MLmetrics::RMSE(log(1 + LE_fits2$mean), log(1 + ytrain))
coa2b_mse <- MLmetrics::RMSE(log(1 + mod2B_preds), log(1 + ytrain))
  
coa0_mse <- MLmetrics::RMSE(log(1 + mod0_preds), log(1 + ytrain))

opar <- par()
par(las = 1)
par(mar = c(5, 6, 4, 2) + 0.1)

tmp <- barplot(c(coa0_mse, coa1a_mse, coa1b_mse, coa2a_mse, coa2b_mse),
        border = F, space = 1,
        horiz = TRUE, names.arg = c('gbm', 'LE means', 'LE meta',
                                    'LE means (iso)', 'LE meta (iso)'),
        cex.names = (0.8), xlim = c(0,.7))


# segments(x0 = 0, y0 = tmp, y1 = tmp, x1 = c(coa0_oos, coa1a_oos, coa1b_oos, coa2a_oos, coa2b_oos),
#        pch = 1, col = 'gray')

points(x = c(coa0_oos, coa1a_oos, coa1b_oos, coa2a_oos, coa2b_oos),
       y = tmp, pch = 15)

abline(v = seq(0, 1, 0.1), col = 'white')

par(opar)
```


# Conclusion

<!-- ## What have I demonstrated? -->

## Which topics require more research?

1. Formulation as a multi-loss network (asynchronous gradient back-prop)
2. Compensation for class imbalance (SMOTE?)
3. Kappa-based optimization methods
4. High-level parallelization

# Questions



