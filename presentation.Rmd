---
title: "Probability calibration methodologies with local expert ensembles"
subtitle: "CPT Nick Normandin"
output: 
  beamer_presentation: 
    colortheme: seahorse
    keep_tex: yes
    theme: Dresden
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library(mlbench)
library(caret)
library(data.table)
library(MLmetrics)
library(localexpeRt)
data("PimaIndiansDiabetes")

ReliabilityPlot <- function(obs, pred, bins = 10) {
  # save and reset default graphical parameters
  opar <- par(no.readonly = TRUE)
  on.exit(par(opar))
  plot(
    c(0, 1),
    c(0, 1),
    col = "grey",
    type = "l",
    xlab = expression('s'[i]),
    ylab = expression('p(c'[i]*')')
  )
  
  # scale
  minp <- min(pred)
  maxp <- max(pred)
  pdiff <- maxp - minp
  pred <- (pred - minp) / pdiff
  
  # bins
  bin.pred <- cut(pred, breaks = bins)
  
  # ratio
  k <- plyr::ldply(levels(bin.pred), function(x) {
    idx <- x == bin.pred
    c(sum(obs[idx]) / length(obs[idx]), mean(pred[idx]))
  })
  
  # replace NAs
  is.nan.idx <- !is.nan(k$V2)
  k <- k[is.nan.idx, ]
  
  lines(k$V2, k$V1, col = 'red', type = 'o', lwd = 4)
}



```

# Introduction

## What should you know about this brief?

> - **Please ask questions as I go**
> - I've assumed some audience proficiency in modern machine learning techniques, but I will alternatively try to provide a heuristic understanding of the concepts presented
> - Full accompanying R code is available
> - This work was funded by the Omar N. Bradley Officer Research Fellowship in Mathematics

## What is `local expert`?

I created a new kind of ensemble forecasting method that I've called local expert regression. It involves the decomposition of a supervised learning task with a continuous target variable (*regression*) into a series of of many $\{0, 1\}$ mappings corresponding to separate *binary probabilistic classification* tasks that produce estimates on the $[0, 1]$ interval.

#####  Why is this useful ?! 
Because you can aggregate the ensemble predictions to form a completely unique *probability distribution function* for each prediction. You can understand **risk** not just in terms of a model, but in terms of each individual forecast.

#### ...see **[github.com/nnormandin/localexpeRt](https://github.com/nnormandin/localexpeRt)**

## What problem am I solving?

\begin{columns}
\begin{column}{0.48\textwidth}

Most classification methods produce scores for class membership which are interpereted as measures of class affiliation probability. This is the foundation of local expert regression. However, these `probabilities' are not usually \textbf{well-calibrated}. 

\end{column}


\begin{column}{0.48\textwidth}
\begin{block}{Definition:}

For a model $f$ and score $s_i$ to be well-calibrated for class $c_i$, the empirical probability of a correct classification $P(c_i | f( c_i | x_i)=s_i)$ must converge to $f(c_i | x_i) = s_i$.

\end{block}
\begin{block}{Example:}

When $s_i = 0.9$, the probability of a correct classification should converge to $P(c_i | s_i = 0.9) = 0.9$. Otherwise, this isn't \textit{really} a `probability.'

\end{block}
\end{column}
\end{columns}



## How do I propose to solve it?

#### If probabilities aren't properly calibrated, the PDFs interpolated from them won't be reliable. How can we deal with this?

1. Change the loss function
     + $n^{-1}\sum_{i=1}^{n}-y_i\log(p_i)-(1-y_i)\log(1-p_i)$
2. Calibrate probabilities
     + isotonic regression, sigmoid transforms?

# Local expert

## How is local expert different from normal regression?

![image](~/R/projects/MORS_2017/figure1/first.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/second.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/third.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/fourth.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/last.png)


## How is local expert different from normal regression?


# Probability calibration

## Why are some model scores poorly calibrated?

\begin{columns}[t]
\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0.5}
\begin{itemize}
\item Maximal margin hyperplanes push scores away from extremes of distribution
\item Common in support vector machines, boosted learners
\end{itemize}
\end{block}

\end{column}


\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0, 1}
\begin{itemize}
\item Model assumptions make class probabilites unrealistically confident
\item Naive Bayes!
\end{itemize}
\end{block}
\end{column}
\end{columns}


## How can we visualize calibration?

Cross-validated class probabilities from a naive bayes model trained on the Pima Indian Diabetes data 

\footnotesize
```{r, message=FALSE, warning=FALSE, cache = TRUE}
m <- train(x = PimaIndiansDiabetes[,1:8], y = PimaIndiansDiabetes[,9], tuneLength = 1,
           method = 'svmLinear', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

pred <- m$pred[order(m$pred$rowIndex),]

result <- data.table(prob = pred$pos,
                     class = ifelse(pred$obs == 'pos', 1, 0))

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')
```

The logarithmic loss here is `r MLmetrics::LogLoss(result$prob, result$class)`

## How can we visualize calibration?

Reliability plot: **(1)** Bin predictions by $s_i$ (x-axis), **(2)** calculate $p(c_i)$ by bin (y-axis)

```{r}
ReliabilityPlot(result$class, result$prob, bins = 10)
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r, cache = TRUE}
x <- seq(200)
y <- seq(200) + rnorm(200, 0, 30)
```

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
lines(isoreg(x,y), lwd = 4)
```

## Method 1: Isotonic Regression
Applying it to the Pima Indian Diabetes estimates from earlier

```{r}
iso_mod <- isoreg(x = result$prob, y = result$class)
ip <- setorder(data.table(iso_mod$yf, iso_mod$ord), V2)
result[, iso_probs := ip$V1]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(iso_mod, lwd = 4)

```

The logarithmic loss here is `r MLmetrics::LogLoss(result$iso_probs, result$class)`


## Method 2: Platt Scaling
Pass $s_i$ through the sigmoid

$$P(c_i | s_i) = \frac{1}{1 + \exp(As_i + B)}$$


where $A$ and $B$ are the solution to
$$\underset{A, B}{\operatorname{argmax}} - \sum\limits_{i} y_i \log(p_i) + (1 - y_i) \log(1- p_i)$$


## Method 2: Platt Scaling
Applying it to the Pima Indian Diabetes estimates from earlier


```{r}

pmod <- train(x = result[,1], y = ifelse(result$class == 1, 'y', 'n'), family = 'binomial',
           method = 'glm', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

result[, platt_probs := pmod$pred[order(pmod$pred$rowIndex),4]]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(y = sort(result$platt_probs), x = sort(result$prob), lwd = 4, col = 'red')
```

# Case study

## The task

- 30,000 records of 300+ variables related to Russian housing transactions
- very dirty, lots of multicollinearity

![image](~/R/projects/MORS_2017/figure1/kaggle.png)

## Preparing the data

I cleaned and pre-processed separately, so we'll just read in those files and partition the train and test sets.

\footnotesize
```{r, echo = TRUE}
X <- readRDS('./dataX')
y <- readRDS('./dataY')

trainset <- createDataPartition(y, p = 0.1)[[1]]

Xtrain <- X[trainset,]; Xtest <- X[-trainset,]
ytrain <- y[trainset]; ytest <- y[-trainset]
```

```{r}
tc <- trainControl(method = 'cv', number = 3, savePredictions = TRUE,
                   classProbs = TRUE, returnData = FALSE)
```


## COA 0: Tune and train a regression model
\footnotesize
```{r, echo = TRUE, message=FALSE, warning=FALSE, cache = TRUE}
mod0 <- train(x = Xtrain, y = ytrain, method = 'gbm',
              tuneLength = 1, trControl = tc,
              verbose = FALSE)
```


## COA 1: Use a local expert ensemble

Map the continuous `y` vector into a binary matrix

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(localexpeRt)
yb <- BinCols(ytrain, n = 8)
```

\normalsize
Induce separate models across each column in the matrix

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
LEs <- TrainLEs(x = Xtrain, bincols = yb$cols, trControl = tc,
                method = 'gbm', n.folds = 3, tuneLength = 1,
                verbose = FALSE)

LE_info <- ExtractModelInfo(LEs)
```

## COA 1: Use a local expert ensemble

\footnotesize
```{r, echo=TRUE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
sink('/dev/null'); PlotLEs(LE_info); sink()
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[1,], yb$y.vals, plot = TRUE)
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[2,], yb$y.vals, plot = TRUE)
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[3,], yb$y.vals, plot = TRUE)
```

## COA 1: Use a local expert ensemble
Predict $\hat{y}$ using just the means of the empirical distributions

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
LE_fits <- FitMatrix(LE_info$preds.matrix, yb$y.vals)
MLmetrics::RMSE(LE_fits$mean, ytrain)
```


## COA 2: Local expert with calibrated probabilities

The output of each local expert must be calibrated independently. Each has a distinct reliability plot, and class imbalance has a substantial effect on the probability scores generated.

```{r, fig.height=5, fig.width=10}
pm <- as.data.table(LE_info$preds.matrix)
c5 <- ifelse(yb$cols$c5 == 'DOWN', 1, 0)
ReliabilityPlot(c5, pm$c5, bins = 10)
```

## COA 2: Local expert with calibrated probabilities

The output of each local expert must be calibrated independently. Each has a distinct reliability plot, and class imbalance has a substantial effect on the probability scores generated.

```{r, fig.height=5, fig.width=10}
c3 <- ifelse(yb$cols$c3 == 'DOWN', 1, 0)
ReliabilityPlot(c3, pm$c5, bins = 10)
```

## COA 2: Local expert with calibrated probabilities

Instead of simply fitting an isotonic regression model to our local expert output, we can remove some of the bumpiness of the step function and reduce overfitting by bootstrapping. The steps are:

1. Draw random sample with replacement
2. Fit isotonic step function using PAVA algorithm
3. Repeat steps 1 & 2 for 500 iterations
4. Average all step functions
5. Return final `step function' (still monotonic)

## COA 2: Local expert with calibrated probabilities
```{r}
bootstrap_iso <- function(preds, y, NBOOT = 500, SAMP = 0.1){
  
  samp_iso <- function(x, y, N, SAMP) {
    rows <- sample(seq(N), as.integer(N * SAMP))
    return(as.stepfun(isoreg(x[rows], y[rows])))
  }
  
  repeated_iso <- function(NBOOT, x, y, N, SAMP){
    return(lapply(seq(NBOOT), function(i) samp_iso(x=x, y=y, N=N, SAMP=SAMP)))
  }
  
  N <- NROW(preds)
  N_LE <- NCOL(preds)
  
  mods <- lapply(seq_along(N_LE), function(x){
    repeated_iso(NBOOT = NBOOT, SAMP = SAMP, N = N,
                 x = preds[,x], y = y[,x])})
  return(mods)
}

predict_bootiso <- function(models, newdata){
  levels <- names(models)
  out <- data.table(tmp = seq(nrow(newdata)))
  
  for (i in levels){
    out[, paste(i) := apply(sapply(models[[i]], function(x){
      x(newdata[[i]])}), 1, mean)]
  }
  
  return(out)
}
```


# Results

## Evaluate model performance

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
pred0 <- predict(mod0, Xtest)

# pred1 <- 

# pred2 <- 

```

# Conclusion

## What have I demonstrated?

## Which topics require more research?

1. Compensation for class imbalance (SMOTE?)
3. Kappa-based optimization methods
4. High-level parallelization

# Questions





