---
title: "Probability calibration methodologies with local expert ensembles"
subtitle: "CPT Nick Normandin"
output: 
  beamer_presentation: 
    colortheme: seahorse
    keep_tex: yes
    theme: Dresden
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(mlbench)
library(caret)
library(data.table)
data("PimaIndiansDiabetes")

ReliabilityPlot <- function(obs, pred, bins = 10) {
  opar <- par(no.readonly = TRUE)
  on.exit(par(opar))
  plot(
    c(0, .5),
    c(0, .5),
    col = "grey",
    type = "l",
    xlab = expression('s'[i]),
    ylab = expression('p(c'[i]*')')
  )
  
  # scale
  minp <- min(pred)
  maxp <- max(pred)
  pdiff <- maxp - minp
  pred <- (pred - minp) / pdiff
  
  # bins
  bin.pred <- cut(pred, breaks = bins)
  
  # ratio
  k <- plyr::ldply(levels(bin.pred), function(x) {
    idx <- x == bin.pred
    c(sum(obs[idx]) / length(obs[idx]), mean(pred[idx]))
  })
  
  # replace NAs
  is.nan.idx <- !is.nan(k$V2)
  k <- k[is.nan.idx, ]
  
  lines(k$V2, k$V1, col = 'red', type = 'o', lwd = 3)
}
```

# Introduction

## What should you know about this brief?

> - **Please ask questions as I go**
> - I've assumed some audience proficiency in modern machine learning techniques, but I will alternatively try to provide a heuristic understanding of the concepts presented
> - Full accompanying R code is available
> - This work was funded by the Omar N. Bradley Officer Research Fellowship in Mathematics

## What is `local expert`?

I created a new kind of ensemble forecasting method that I've called local expert regression. It involves the decomposition of a supervised learning task with a continuous target variable (*regression*) into a series of of many $\{0, 1\}$ mappings corresponding to separate *binary probabilistic classification* tasks that produce estimates on the $[0, 1]$ interval.

#####  Why is this useful ?! 
Because you can aggregate the ensemble predictions to form a completely unique *probability distribution function* for each prediction. You can understand **risk** not just in terms of a model, but in terms of each individual forecast.

#### ...see **[github.com/nnormandin/localexpeRt](https://github.com/nnormandin/localexpeRt)**

<!-- ## What problem am I solving? -->

<!-- Most classification methods produce scores for class membership which are interpereted as measures of class affiliation probability. This is the foundation of local expert regression. However, these 'probabilities' are not usually **well-calibrated**.  -->
 
<!-- #### Definition -->
<!-- For a model $f$ and score $s_i$ to be well-calibrated for class $c_i$, the empirical probability of a correct classification $P(c_i | f( c_i | x_i)=s_i)$ must converge to $f(c_i | x_i) = s_i$. -->

<!-- #### Example -->
<!-- When $s_i = 0.9$, the probability of a correct classification should converge to $P(c_i | s_i = 0.9) = 0.9$. Otherwise, this isn't *really* a 'probability.' -->

## What problem am I solving?

\begin{columns}
\begin{column}{0.48\textwidth}

Most classification methods produce scores for class membership which are interpereted as measures of class affiliation probability. This is the foundation of local expert regression. However, these `probabilities' are not usually \textbf{well-calibrated}. 

\end{column}


\begin{column}{0.48\textwidth}
\begin{block}{Definition:}

For a model $f$ and score $s_i$ to be well-calibrated for class $c_i$, the empirical probability of a correct classification $P(c_i | f( c_i | x_i)=s_i)$ must converge to $f(c_i | x_i) = s_i$.

\end{block}
\begin{block}{Example:}

When $s_i = 0.9$, the probability of a correct classification should converge to $P(c_i | s_i = 0.9) = 0.9$. Otherwise, this isn't \textit{really} a `probability.'

\end{block}
\end{column}
\end{columns}



## How do I propose to solve it?

#### If probabilities aren't properly calibrated, the PDFs interpolated from them won't be reliable. How can we deal with this?

1. Change the loss function
     + $n^{-1}\sum_{i=1}^{n}-y_i\log(p_i)-(1-y_i)\log(1-p_i)$
2. Calibrate probabilities
     + isotonic regression, sigmoid transforms?

# Local expert

## How is local expert different from normal regression?

![image](~/R/projects/MORS_2017/figure1/first.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/second.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/third.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/fourth.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/last.png)


## How is local expert different from normal regression?


# Probability calibration

## Why are some model scores poorly calibrated?

\begin{columns}[t]
\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0.5}
\begin{itemize}
\item Maximal margin hyperplanes push scores away from extremes of distribution
\item Common in support vector machines, boosted learners
\end{itemize}
\end{block}

\end{column}


\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0, 1}
\begin{itemize}
\item Model assumptions make class probabilites unrealistically confident
\item Naive Bayes!
\end{itemize}
\end{block}
\end{column}
\end{columns}


## How can we visualize calibration?

Cross-validated class probabilities from a naive bayes model trained on the Pima Indian Diabetes data 

\footnotesize
```{r, message=FALSE, warning=FALSE}
m <- train(x = PimaIndiansDiabetes[,1:8], y = PimaIndiansDiabetes[,9],
           method = 'nb', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

result <- data.table(prob = m$pred$pos,
                     class = ifelse(m$pred$obs == 'pos', 1, 0))

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')
```

## How can we visualize calibration?

Reliability plot: **(1)** Bin predictions by $s_i$ (x-axis), **(2)** calculate $p(c_i)$ by bin (y-axis)

```{r}
ReliabilityPlot(result$class, result$prob, bins = 10)
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r}
x <- seq(200)
y <- seq(200) + rnorm(200, 0, 20)
```

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
lines(isoreg(x,y), lwd = 4)
```

## Method 2: Platt Scaling


# Case study

# Results



# Conclusion

## What have I demonstrated?

## Which topics require more research?

1. Compensation for class imbalance (SMOTE?)
3. Kappa-based optimization methods
4. High-level parallelization

# Questions





