---
title: "Probability calibration methodologies with local expert ensembles"
subtitle: "CPT Nick Normandin"
output: 
  beamer_presentation: 
    colortheme: seahorse
    keep_tex: yes
    theme: Dresden
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(mlbench)
library(caret)
library(data.table)
library(MLmetrics)
library(localexpeRt)
data("PimaIndiansDiabetes")

ReliabilityPlot <- function(obs, pred, bins = 10) {
  # save and reset default graphical parameters
  opar <- par(no.readonly = TRUE)
  on.exit(par(opar))
  plot(
    c(0, 1),
    c(0, 1),
    col = "grey",
    type = "l",
    xlab = expression('s'[i]),
    ylab = expression('p(c'[i]*')')
  )
  
  # scale
  minp <- min(pred)
  maxp <- max(pred)
  pdiff <- maxp - minp
  pred <- (pred - minp) / pdiff
  
  # bins
  bin.pred <- cut(pred, breaks = bins)
  
  # ratio
  k <- plyr::ldply(levels(bin.pred), function(x) {
    idx <- x == bin.pred
    c(sum(obs[idx]) / length(obs[idx]), mean(pred[idx]))
  })
  
  # replace NAs
  is.nan.idx <- !is.nan(k$V2)
  k <- k[is.nan.idx, ]
  
  lines(k$V2, k$V1, col = 'red', type = 'o', lwd = 4)
}



```

# Introduction

## What should you know about this brief?

> - **Please ask questions as I go**
> - I've assumed some audience proficiency in modern machine learning techniques, but I will alternatively try to provide a heuristic understanding of the concepts presented
> - Full accompanying R code is available
> - This work was funded by the Omar N. Bradley Officer Research Fellowship in Mathematics

## What is `local expert`?

I created a new kind of ensemble forecasting method that I've called local expert regression. It involves the decomposition of a supervised learning task with a continuous target variable (*regression*) into a series of of many $\{0, 1\}$ mappings corresponding to separate *binary probabilistic classification* tasks that produce estimates on the $[0, 1]$ interval.

#####  Why is this useful ?! 
Because you can aggregate the ensemble predictions to form a completely unique *probability distribution function* for each prediction. You can understand **risk** not just in terms of a model, but in terms of each individual forecast.

#### ...see **[github.com/nnormandin/localexpeRt](https://github.com/nnormandin/localexpeRt)**

## What problem am I solving?

\begin{columns}
\begin{column}{0.48\textwidth}

Most classification methods produce scores for class membership which are interpereted as measures of class affiliation probability. This is the foundation of local expert regression. However, these `probabilities' are not usually \textbf{well-calibrated}. 

\end{column}


\begin{column}{0.48\textwidth}
\begin{block}{Definition:}

For a model $f$ and score $s_i$ to be well-calibrated for class $c_i$, the empirical probability of a correct classification $P(c_i | f( c_i | x_i)=s_i)$ must converge to $f(c_i | x_i) = s_i$.

\end{block}
\begin{block}{Example:}

When $s_i = 0.9$, the probability of a correct classification should converge to $P(c_i | s_i = 0.9) = 0.9$. Otherwise, this isn't \textit{really} a `probability.'

\end{block}
\end{column}
\end{columns}



## How do I propose to solve it?

#### If probabilities aren't properly calibrated, the PDFs interpolated from them won't be reliable. How can we deal with this?

1. Change the loss function
     + $n^{-1}\sum_{i=1}^{n}-y_i\log(p_i)-(1-y_i)\log(1-p_i)$
2. Calibrate probabilities
     + isotonic regression, sigmoid transforms?

# Local expert

## How is local expert different from normal regression?

![image](~/R/projects/MORS_2017/figure1/first.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/second.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/third.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/fourth.png)

## How is local expert different from normal regression?
![image](~/R/projects/MORS_2017/figure1/last.png)


## How is local expert different from normal regression?


# Probability calibration

## Why are some model scores poorly calibrated?

\begin{columns}[t]
\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0.5}
\begin{itemize}
\item Maximal margin hyperplanes push scores away from extremes of distribution
\item Common in support vector machines, boosted learners
\end{itemize}
\end{block}

\end{column}


\begin{column}{0.48\textwidth}

\begin{block}{$s_i$ dense around 0, 1}
\begin{itemize}
\item Model assumptions make class probabilites unrealistically confident
\item Naive Bayes!
\end{itemize}
\end{block}
\end{column}
\end{columns}


## How can we visualize calibration?

Cross-validated class probabilities from a naive bayes model trained on the Pima Indian Diabetes data 

\footnotesize
```{r, message=FALSE, warning=FALSE, cache = TRUE}
m <- train(x = PimaIndiansDiabetes[,1:8], y = PimaIndiansDiabetes[,9], tuneLength = 1,
           method = 'svmLinear', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

pred <- m$pred[order(m$pred$rowIndex),]

result <- data.table(prob = pred$pos,
                     class = ifelse(pred$obs == 'pos', 1, 0))

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')
```

The logarithmic loss here is `r MLmetrics::LogLoss(result$prob, result$class)`

## How can we visualize calibration?

Reliability plot: **(1)** Bin predictions by $s_i$ (x-axis), **(2)** calculate $p(c_i)$ by bin (y-axis)

```{r}
ReliabilityPlot(result$class, result$prob, bins = 10)
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r, cache = TRUE}
x <- seq(200)
y <- seq(200) + rnorm(200, 0, 30)
```

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
```

## Method 1: Isotonic Regression
A strictly-nondecreasing piecewise linear function $m$, where $y_i = m(s_i) + \epsilon$ fit such that $\hat{m} = {argmin}_z \sum_i{y_i-z(s_i) ^2}$.

```{r}
plot(x, y, bty = 'n', yaxt = 'n', xaxt = 'n', ylab = '', xlab = '')
lines(isoreg(x,y), lwd = 4)
```

## Method 1: Isotonic Regression
Applying it to the Pima Indian Diabetes estimates from earlier

```{r}
iso_mod <- isoreg(x = result$prob, y = result$class)
ip <- setorder(data.table(iso_mod$yf, iso_mod$ord), V2)
result[, iso_probs := ip$V1]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(iso_mod, lwd = 4)

```

The logarithmic loss here is `r MLmetrics::LogLoss(result$iso_probs, result$class)`


## Method 2: Platt Scaling
Pass $s_i$ through the sigmoid

$$P(c_i | s_i) = \frac{1}{1 + \exp(As_i + B)}$$


where $A$ and $B$ are the solution to
$$\underset{A, B}{\operatorname{argmax}} - \sum\limits_{i} y_i \log(p_i) + (1 - y_i) \log(1- p_i)$$


## Method 2: Platt Scaling
Applying it to the Pima Indian Diabetes estimates from earlier


```{r}

pmod <- train(x = result[,1], y = ifelse(result$class == 1, 'y', 'n'), family = 'binomial',
           method = 'glm', trControl = trainControl(method = 'cv',
                                                   savePredictions = TRUE,
                                                   classProbs = TRUE))

result[, platt_probs := pmod$pred[order(pmod$pred$rowIndex),4]]

plot(x = result$prob, y = result$class, pch = '|',
     xlab = 'p(y = 1)', ylab = '', bty = 'n', yaxt = 'n')

lines(y = sort(result$platt_probs), x = sort(result$prob), lwd = 4, col = 'red')
```

# Case study

## The task

- 30,000 records of 300+ variables related to Russian housing transactions
- very dirty, lots of multicollinearity

![image](~/R/projects/MORS_2017/figure1/kaggle.png)

## Preparing the data

I cleaned and pre-processed separately, so we'll just read in those files and partition the train and test sets.

\footnotesize
```{r, echo = TRUE}
X <- readRDS('./dataX')
y <- readRDS('./dataY')

trainset <- createDataPartition(y, p = 0.1)[[1]]

Xtrain <- X[trainset,]; Xtest <- X[-trainset,]
ytrain <- y[trainset]; ytest <- y[-trainset]
```

```{r}
tc <- trainControl(method = 'cv', number = 3, savePredictions = TRUE,
                   classProbs = TRUE, returnData = FALSE)
```


## COA 0: Tune and train a regression model
\footnotesize
```{r, echo = TRUE, message=FALSE, warning=FALSE, cache = TRUE}
mod0 <- train(x = Xtrain, y = ytrain, method = 'gbm',
              tuneLength = 1, trControl = tc,
              verbose = FALSE)
```


## COA 1: Use a local expert ensemble

Map the continuous `y` vector into a binary matrix

\footnotesize
```{r, echo = TRUE}
library(localexpeRt)
yb <- BinCols(ytrain, n = 8)
```

\normalsize
Induce separate models across each column in the matrix

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
LEs <- TrainLEs(x = Xtrain, bincols = yb$cols, trControl = tc,
                method = 'gbm', n.folds = 3, tuneLength = 1,
                verbose = FALSE)

LE_info <- ExtractModelInfo(LEs)
```

## COA 1: Use a local expert ensemble

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
PlotLEs(LE_info)
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[1,], yb$y.vals, plot = TRUE)
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[2,], yb$y.vals, plot = TRUE)
```

## COA 1: Use a local expert ensemble
Now for each separate instance, we're predicting a distribution instead of a value:

\footnotesize
```{r, echo=TRUE, message=FALSE, warning=FALSE}
FitInstance(LE_info$preds.matrix[3,], yb$y.vals, plot = TRUE)
```


## COA 2: Local expert with calibrated probabilities



# Results

## Evaluate model performance

\footnotesize
```{r, echo = TRUE}
pred0 <- predict(mod0, Xtest)

# pred1 <- 

# pred2 <- 

```

# Conclusion

## What have I demonstrated?

## Which topics require more research?

1. Compensation for class imbalance (SMOTE?)
3. Kappa-based optimization methods
4. High-level parallelization

# Questions





